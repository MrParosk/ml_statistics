{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectationâ€“maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM-algorithm is a method for doing MLE estimation when we have unobserved variables.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $x_i$, $i=1,...,n$ be the observed variables.\n",
    "- $z_i$, $i=1,...,n$ be the unobserved variables.\n",
    "- $\\theta$ be the parameter of interest where $\\theta \\in \\Theta$.\n",
    "\n",
    "For the problems EM tries to solve we have that $p(x,z) \\sim p_{\\theta}$ where $\\theta$ is unknown.\n",
    "\n",
    "One naive approach for estimating $\\theta$ is \"regular\" MLE:\n",
    "\n",
    "$$\\theta_{mle} = \\text{argmax}_{\\theta} p(x | \\theta) =\\text{argmax}_{\\theta} \\sum_z p(x,z | \\theta) $$\n",
    "\n",
    "here we have assumed that $z$ is discrete. However, $\\sum_{z} p(x,z | \\theta)$ can be hard/impossible to compute. But we can maximize $p(x|\\theta)$ indirectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm\n",
    "\n",
    "Initilize $\\theta^{(0)} = \\theta_0$. For $t=1,...,T$:\n",
    "\n",
    "- 1: $Q(\\theta, \\theta^{(t)}) = E_{z | x, \\theta^{(t)}} [log p(x,z|\\theta)]$.\n",
    "- 2: $\\theta^{(t+1)} = \\text{argmax}_{\\theta} Q(\\theta, \\theta^{(t)})$\n",
    "\n",
    "Below we derive why this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation-maximization works to improve $Q(\\theta|\\theta^{(t)})$ rather than directly improving $\\log p(x|\\theta)$. We will now prove that improving $Q(\\theta|\\theta^{(t)})$ implies improving $\\log p(x|\\theta)$. Using the product rule we get:\n",
    "\n",
    "$$\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) $$\n",
    "\n",
    "Now we multiplying both sides by $p(z|x,\\theta^{(t)})$ and summing over $z$. This yields:\n",
    "\n",
    "$$\\log p(x|\\theta) = \\sum_{z} p(z|x,\\theta^{(t)}) \\log p(x,z|\\theta) - \\sum_{z} p(z|x,\\theta^{(t)}) \\log p(z|x,\\theta) = Q(\\theta|\\theta^{(t)}) + H(\\theta|\\theta^{(t)})$$\n",
    "\n",
    "note that $\\log p(x|\\theta)$ stay the same since it's independent of $z$ and $\\sum_{z} p(z|x,\\theta^{(t)}) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last equation holds for any value of $\\theta$, so if $\\theta = \\theta^{(t)}$ then:\n",
    "\n",
    "$$\\log p(x|\\theta^{(t)}) = Q(\\theta^{(t)}|\\theta^{(t)}) + H(\\theta^{(t)}|\\theta^{(t)}) $$\n",
    "\n",
    "Subtracting this last equation from the previous equation gives\n",
    "\n",
    "$$\\log p(x|\\theta) - \\log p(x|\\theta^{(t)}) = Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) + H(\\theta|\\theta^{(t)}) - H(\\theta^{(t)}|\\theta^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Gibbs' inequality we have $H(\\theta|\\theta^{(t)}) \\ge H(\\theta^{(t)}|\\theta^{(t)})$, i.e:\n",
    "\n",
    "$$\\log p(x|\\theta) - \\log p(x|\\theta^{(t)}) \\ge Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})$$\n",
    "\n",
    "So we conclude that choosing $\\theta$ to improve $Q(\\theta|\\theta^{(t)})$ w.r.t. $Q(\\theta^{(t)}|\\theta^{(t)})$ cannot cause $\\log p(x|\\theta)$ to decrease w.r.t. $\\log p(x|\\theta^{(t)})$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
