{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't compute the posterior distribution for many (interesting) models. We then need to use approximate methods, such as variational inference. Here we want to infer hidden variables.\n",
    "\n",
    "2 - Unlike MCMC VI is:\n",
    "- Deterministic.\n",
    "- Easy to gauge convergence.\n",
    "- Requires dozens of iterations.\n",
    "\n",
    "Also it doesn't require conjugacy. Now let:\n",
    "\n",
    "- x be the observed variables.\n",
    "- z be the hidden variables.\n",
    "- $\\theta$ be the parameters in the model.\n",
    "\n",
    "We want the posterior distribution:\n",
    "\n",
    "$$p(z | x, \\theta) = \\frac{p(z, x | \\theta)}{\\int p(z, x | \\theta) dz}$$\n",
    "\n",
    "However $\\int p(z, x | \\theta) dz$ may be intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea\n",
    "\n",
    "Create a variational distribution over the latent variables, i.e. $q(z | v)$ where v is the variational parameters.\n",
    "\n",
    "Find settings of v s.t. q is close to the posterior $p(z | x, \\theta)$, i.e. we want $q(z|x)$ s.t. $KL(q(|x) | p(z|x))$ is small. It can be shown that (using the definition of conditional probability and KL):\n",
    "\n",
    "$$\\log(p(x)) = KL(q(|x) | p(z|x)) + E(log(p(x|z))) - KL(q(z|x) | q(z)) = KL(q(|x) | p(z|x)) + \\text{ELBO}$$\n",
    "\n",
    "where $\\text{ELBO} = E(log(p(x|z))) - KL(q(z|x) | q(z))$ and stands for (Evidence Lower Bound). We note that $\\log(p(x))$ is independent of $q$, so if we maximize ELBO then we minimize $KL(q(|x) | p(z|x))$ since they are constant w.r.t. q. This is our original goal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean field VI\n",
    "\n",
    "One common assumption is that the variational distribution factorizes:\n",
    "\n",
    "$$p(z_1, ..., z_m) = \\Pi_{j=1}^m p(z_j) $$\n",
    "\n",
    "You may want to group some hidden variables togheter. Does not contain the true posterior distribution because the hidden variables are dependent, however can be a good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General blueprint for VI\n",
    "\n",
    "- Choose a distribution q.\n",
    "- Derive ELBO.\n",
    "- Coordinate ascent for each $q_i$.\n",
    "- Repeat until convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
