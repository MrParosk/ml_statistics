{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't compute the posterior distribution for many (interesting) models. We then need to use approximate methods, such as variational inference. Here we want to infer hidden variables.\n",
    "\n",
    "2 - Unlike MCMC VI is:\n",
    "- Deterministic.\n",
    "- Easy to gauge convergence.\n",
    "- Requires dozens of iterations.\n",
    "\n",
    "Also it doesn't require conjugacy. Now let:\n",
    "\n",
    "- x be the observed variables.\n",
    "- z be the hidden variables.\n",
    "- $\\theta$ be the parameters in the model.\n",
    "\n",
    "We want the posterior distribution:\n",
    "\n",
    "$$p(z | x, \\theta) = \\frac{p(z, x | \\theta)}{\\int p(z, x | \\theta) dz}$$\n",
    "\n",
    "However $\\int p(z, x | \\theta) dz$ may be intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea\n",
    "\n",
    "Create a variational distribution over the latent variables, i.e. $q(z | v)$ where v is the variational parameters.\n",
    "\n",
    "Find settings of v s.t. q is close to the posterior $p(z | x, \\theta)$.\n",
    "\n",
    "q is found by minimizing the KL-divergence, i.e.\n",
    "\n",
    "$$KL(q(z) | p(z|x)) = ... = -(E_q[ log(p(x,z))] - E_q[ log(q(z))] ) + C $$\n",
    "\n",
    "this is derived with the definition of conditional probability. Note that minimizing KL-divergence is the same as maximizing the ELBO (Evidence Lower Bound): $\\text{ELBO} = E_q[ log(p(x,z))] - E_q[ log(q(z))]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean field VI\n",
    "\n",
    "One common assumption is that the variational distribution factorizes:\n",
    "\n",
    "$$p(z_1, ..., z_m) = \\Pi_{j=1}^m p(z_j) $$\n",
    "\n",
    "You may want to group some hidden variables togheter. Does not contain the true posterior distribution because the hidden variables are dependent, however can be a good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General blueprint for VI\n",
    "\n",
    "- Choose a distribution q.\n",
    "- Derive ELBO (or KL-divergence).\n",
    "- Coordinate ascent for each $q_i$.\n",
    "- Repeat until convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
